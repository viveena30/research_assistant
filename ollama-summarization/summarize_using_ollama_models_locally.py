import pdfplumber
import fitz  # pymupdf
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import ollama

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Paths
PDF_PATH = "./pdfs/done - Virtual Machine Consolidation with Minimization of Migration - 2020.pdf"
OUTPUT_PATH = "cleaned_output.txt"
# 
# Extract text using PyMuPDF (fast + consistent encoding)
def extract_text_pymupdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text("text") + "\n"
    return text

# Preprocessing
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

# Remove repeated headers/footers
def get_repeated_lines(pages_text, num_lines=2):
    headers, footers = [], []
    for text in pages_text:
        lines = text.split("\n")
        if len(lines) >= num_lines:
            headers.append(tuple(lines[:num_lines]))
            footers.append(tuple(lines[-num_lines:]))
    header_counts = Counter(headers)
    footer_counts = Counter(footers)
    common_headers = {k for k, v in header_counts.items() if v > len(pages_text) * 0.8}
    common_footers = {k for k, v in footer_counts.items() if v > len(pages_text) * 0.8}
    return common_headers, common_footers

def extract_clean_text(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        raw_pages = [page.extract_text() or "" for page in pdf.pages]

    headers, footers = get_repeated_lines(raw_pages)
    cleaned_pages = []
    for text in raw_pages:
        lines = text.split("\n")
        if len(lines) >= 2:
            if tuple(lines[:2]) in headers:
                lines = lines[2:]
            if tuple(lines[-2:]) in footers:
                lines = lines[:-2]
        cleaned_pages.append("\n".join(lines))

    return "\n".join(cleaned_pages)

# Remove everything after "References"
def remove_references_section(text):
    pattern = re.compile(r"(?i)^references\s*$", re.MULTILINE)
    match = pattern.search(text)
    if match:
        return text[:match.start()].strip()
    return text

# Deduplicate sentences
def deduplicate_sentences(text):
    seen = set()
    sentences = re.split(r"(?<=[.?!])\s+", text)
    unique = []
    for sentence in sentences:
        clean = sentence.strip()
        if clean and clean not in seen:
            seen.add(clean)
            unique.append(clean)
    return " ".join(unique)

# Full pipeline
def clean_pdf(pdf_path):
    print(f"Cleaning: {pdf_path}")
    extracted_text = extract_text_pymupdf(pdf_path)
    cleaned_text = remove_references_section(extracted_text)
    cleaned_text = deduplicate_sentences(cleaned_text)
    return cleaned_text

def llama_summarize_text(text: str, model_name: str = 'llama3') -> str:
    """
    Prerequisites: ollama run model_name
    
    Args:
        text (str): The text to be summarized.
        model_name (str): The model to use ('llama3' for Llama 3 or 'mixtral' for Mixtral 8x7B).

    Returns:
        str: The summary generated by the model.
    """
    try:
        # can modify the prompt based on requirement
        prompt = f"Summarize the following research content:\n\n{text}"
        response = ollama.chat(
            model=model_name,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return response['message']['content']
    except Exception as e:
        return f"Error during summarization: {e}"

if __name__ == "__main__":
    cleaned_text = clean_pdf(PDF_PATH)

    # Optional: save cleaned text
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(cleaned_text)
    print(f"Cleaned text saved to: {OUTPUT_PATH}")

    # Generate summary
    print("Generating summary...\n")
    # final_summary = summarize_text(cleaned_text)
    model_to_use = 'llama3.2'
    llama_summary = llama_summarize_text(cleaned_text, model_name=model_to_use)
    print("\nSummary:\n")
    print(llama_summary)
